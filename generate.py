"""
Script de g√©n√©ration audio (doublage) √† partir de sous-titres.
Utilise Qwen3-TTS 0.6B pour synth√©tiser la voix.

Pr√©requis:
    pip install -U qwen-tts soundfile numpy tqdm
    # Optionnel pour performance GPU:
    pip install -U flash-attn --no-build-isolation
"""

import sys
import argparse
import re
from pathlib import Path
from typing import Optional
from dataclasses import dataclass

try:
    import torch
    import soundfile as sf
    import numpy as np
    from tqdm import tqdm
except ImportError as e:
    print(f"‚ùå D√©pendance manquante: {e}")
    print("üí° Installez: pip install torch soundfile numpy tqdm")
    sys.exit(1)

# Langues support√©es par Qwen3-TTS
TTS_LANGUAGES = {
    "fr": "French",
    "en": "English", 
    "ja": "Japanese",
    "zh": "Chinese",
    "ko": "Korean",
    "de": "German",
    "es": "Spanish",
    "it": "Italian",
    "pt": "Portuguese",
    "ru": "Russian",
}


@dataclass
class SubtitleSegment:
    """Repr√©sente un segment de sous-titre."""
    index: int
    start: float
    end: float
    text: str


def parse_srt_time(time_str: str) -> float:
    """Convertit un timestamp SRT en secondes."""
    # Format: HH:MM:SS,mmm
    match = re.match(r'(\d{2}):(\d{2}):(\d{2})[,.](\d{3})', time_str)
    if match:
        h, m, s, ms = map(int, match.groups())
        return h * 3600 + m * 60 + s + ms / 1000
    return 0.0


def parse_srt(srt_path: Path) -> list[SubtitleSegment]:
    """Parse un fichier SRT et retourne une liste de segments."""
    segments = []
    
    with open(srt_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    # S√©parer les blocs par double retour √† la ligne
    blocks = re.split(r'\n\n+', content.strip())
    
    for block in blocks:
        lines = block.strip().split('\n')
        if len(lines) >= 3:
            try:
                index = int(lines[0])
                
                # Parser les timestamps
                time_match = re.match(
                    r'(\d{2}:\d{2}:\d{2}[,.]\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2}[,.]\d{3})',
                    lines[1]
                )
                if time_match:
                    start = parse_srt_time(time_match.group(1))
                    end = parse_srt_time(time_match.group(2))
                    text = ' '.join(lines[2:]).strip()
                    
                    if text:  # Ignorer les segments vides
                        segments.append(SubtitleSegment(
                            index=index,
                            start=start,
                            end=end,
                            text=text
                        ))
            except ValueError:
                continue
    
    return segments


def load_tts_model(device: str = "auto", use_flash_attn: bool = True):
    """
    Charge le mod√®le Qwen3-TTS.
    
    Args:
        device: "cuda", "cpu", ou "auto"
        use_flash_attn: Utiliser Flash Attention 2 (plus rapide sur GPU)
    """
    try:
        from qwen_tts import Qwen3TTSModel
    except ImportError:
        print("‚ùå qwen-tts n'est pas install√©")
        print("üí° Installez: pip install -U qwen-tts")
        sys.exit(1)
    
    # D√©terminer le device
    if device == "auto":
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
    
    print(f"üîß Chargement du mod√®le Qwen3-TTS-0.6B sur {device}...")
    
    # Configuration du mod√®le
    model_kwargs = {
        "device_map": device,
    }
    
    if device.startswith("cuda"):
        model_kwargs["dtype"] = torch.bfloat16
        if use_flash_attn:
            try:
                model_kwargs["attn_implementation"] = "flash_attention_2"
                print("   ‚ö° Flash Attention 2 activ√©")
            except Exception:
                print("   ‚ö†Ô∏è Flash Attention non disponible, utilisation standard")
    else:
        model_kwargs["dtype"] = torch.float32
        print("   ‚ö†Ô∏è CPU d√©tect√© - la g√©n√©ration sera plus lente")
    
    model = Qwen3TTSModel.from_pretrained(
        "Qwen/Qwen3-TTS-12Hz-0.6B-Base",
        **model_kwargs
    )
    
    print("   ‚úÖ Mod√®le charg√©")
    return model


def generate_segment_audio(
    model, 
    text: str, 
    language: str,
    ref_audio: Optional[str] = None,
    ref_text: Optional[str] = None
) -> tuple:
    """
    G√©n√®re l'audio pour un segment de texte.
    
    Args:
        model: Mod√®le Qwen3-TTS
        text: Texte √† synth√©tiser
        language: Langue (en anglais, ex: "French")
        ref_audio: Audio de r√©f√©rence pour le clonage vocal (optionnel)
        ref_text: Texte de l'audio de r√©f√©rence (optionnel)
    
    Returns:
        Tuple (audio_array, sample_rate)
    """
    if ref_audio and ref_text:
        # Mode clonage vocal
        wavs, sr = model.generate_voice_clone(
            text=text,
            language=language,
            ref_audio=ref_audio,
            ref_text=ref_text,
        )
    else:
        # Mode synth√®se standard
        wavs, sr = model.generate(
            text=text,
            language=language,
        )
    
    return wavs[0], sr


def generate_dubbed_audio(
    srt_path: Path,
    output_path: Path,
    language: str = "fr",
    ref_audio: Optional[str] = None,
    ref_text: Optional[str] = None,
    device: str = "auto"
) -> Path:
    """
    G√©n√®re un fichier audio doubl√© √† partir des sous-titres.
    
    Args:
        srt_path: Chemin vers le fichier SRT
        output_path: Chemin de sortie pour l'audio
        language: Code langue (fr, en, ja, etc.)
        ref_audio: Audio de r√©f√©rence pour clonage vocal
        ref_text: Texte de l'audio de r√©f√©rence
        device: Device pour le mod√®le
    
    Returns:
        Chemin vers le fichier audio g√©n√©r√©
    """
    # V√©rifier la langue
    lang_name = TTS_LANGUAGES.get(language)
    if not lang_name:
        print(f"‚ùå Langue '{language}' non support√©e par TTS")
        print(f"üí° Langues disponibles: {', '.join(TTS_LANGUAGES.keys())}")
        sys.exit(1)
    
    print(f"üéôÔ∏è G√©n√©ration de doublage")
    print(f"üìÑ Sous-titres: {srt_path}")
    print(f"üåç Langue: {lang_name}")
    print(f"üìÅ Sortie: {output_path}")
    
    # Parser les sous-titres
    segments = parse_srt(srt_path)
    if not segments:
        print("‚ùå Aucun segment trouv√© dans le fichier SRT")
        sys.exit(1)
    
    print(f"üìù {len(segments)} segments √† g√©n√©rer")
    
    # Charger le mod√®le
    model = load_tts_model(device)
    
    # D√©terminer la dur√©e totale (dernier segment + marge)
    total_duration = max(seg.end for seg in segments) + 1.0
    sample_rate = 24000  # Sera mis √† jour par le mod√®le
    
    # G√©n√©rer audio pour chaque segment
    audio_segments = []
    
    for seg in tqdm(segments, desc="üîä G√©n√©ration"):
        try:
            audio, sr = generate_segment_audio(
                model=model,
                text=seg.text,
                language=lang_name,
                ref_audio=ref_audio,
                ref_text=ref_text
            )
            sample_rate = sr
            audio_segments.append({
                'start': seg.start,
                'end': seg.end,
                'audio': audio,
                'text': seg.text
            })
        except Exception as e:
            print(f"\n‚ö†Ô∏è Erreur segment {seg.index}: {e}")
            # Cr√©er un segment silencieux
            silence_duration = seg.end - seg.start
            silence = np.zeros(int(silence_duration * sample_rate))
            audio_segments.append({
                'start': seg.start,
                'end': seg.end,
                'audio': silence,
                'text': seg.text
            })
    
    # Assembler l'audio final avec timing correct
    print("\nüîß Assemblage de l'audio final...")
    
    # Cr√©er un buffer pour toute la dur√©e
    total_samples = int(total_duration * sample_rate)
    final_audio = np.zeros(total_samples)
    
    for seg_data in audio_segments:
        start_sample = int(seg_data['start'] * sample_rate)
        audio = seg_data['audio']
        
        # Calculer la dur√©e disponible
        available_duration = seg_data['end'] - seg_data['start']
        available_samples = int(available_duration * sample_rate)
        
        # Ajuster la longueur de l'audio si n√©cessaire
        if len(audio) > available_samples:
            # L'audio est trop long, on le coupe
            audio = audio[:available_samples]
        elif len(audio) < available_samples:
            # L'audio est trop court, on ajoute du silence
            padding = np.zeros(available_samples - len(audio))
            audio = np.concatenate([audio, padding])
        
        # Ins√©rer dans le buffer final
        end_sample = start_sample + len(audio)
        if end_sample <= total_samples:
            # Mixer avec l'existant (au cas o√π il y a overlap)
            final_audio[start_sample:end_sample] += audio
    
    # Normaliser pour √©viter le clipping
    max_val = np.max(np.abs(final_audio))
    if max_val > 0:
        final_audio = final_audio / max_val * 0.9
    
    # Sauvegarder
    sf.write(str(output_path), final_audio, sample_rate)
    
    print(f"‚úÖ Audio doubl√© g√©n√©r√©: {output_path}")
    print(f"   ‚è±Ô∏è Dur√©e: {total_duration:.1f}s")
    print(f"   üéµ Sample rate: {sample_rate}Hz")
    
    return output_path


def main():
    parser = argparse.ArgumentParser(
        description="G√©n√®re un doublage audio √† partir de sous-titres avec Qwen3-TTS"
    )
    parser.add_argument(
        "srt_file", 
        type=str, 
        help="Fichier SRT contenant les sous-titres traduits"
    )
    parser.add_argument(
        "-o", "--output",
        type=str,
        default=None,
        help="Fichier audio de sortie (d√©faut: {nom}_dubbed.wav)"
    )
    parser.add_argument(
        "-l", "--language",
        type=str,
        default="fr",
        choices=list(TTS_LANGUAGES.keys()),
        help="Langue des sous-titres (d√©faut: fr)"
    )
    parser.add_argument(
        "--ref-audio",
        type=str,
        default=None,
        help="Audio de r√©f√©rence pour le clonage vocal"
    )
    parser.add_argument(
        "--ref-text",
        type=str,
        default=None,
        help="Texte prononc√© dans l'audio de r√©f√©rence"
    )
    parser.add_argument(
        "-d", "--device",
        type=str,
        default="auto",
        choices=["auto", "cuda:0", "cuda:1", "cpu"],
        help="Device pour le mod√®le (d√©faut: auto)"
    )
    parser.add_argument(
        "--list-voices",
        action="store_true",
        help="Afficher les langues support√©es"
    )
    
    args = parser.parse_args()
    
    if args.list_voices:
        print("üåç Langues support√©es par Qwen3-TTS:")
        for code, name in TTS_LANGUAGES.items():
            print(f"   {code}: {name}")
        return
    
    srt_path = Path(args.srt_file)
    
    if not srt_path.exists():
        print(f"‚ùå Fichier introuvable: {srt_path}")
        sys.exit(1)
    
    # D√©finir le fichier de sortie
    if args.output:
        output_path = Path(args.output)
    else:
        output_path = srt_path.with_name(f"{srt_path.stem}_dubbed.wav")
    
    # V√©rifier les param√®tres de clonage vocal
    if (args.ref_audio and not args.ref_text) or (args.ref_text and not args.ref_audio):
        print("‚ùå Pour le clonage vocal, --ref-audio et --ref-text sont tous deux requis")
        sys.exit(1)
    
    generate_dubbed_audio(
        srt_path=srt_path,
        output_path=output_path,
        language=args.language,
        ref_audio=args.ref_audio,
        ref_text=args.ref_text,
        device=args.device
    )


if __name__ == "__main__":
    main()
